{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image # Read and resize image\n# Import library and dataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import colors\nfrom matplotlib.ticker import PercentFormatter\nimport os","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n\n# https://auth0.com/blog/image-processing-in-python-with-pillow/#:~:text=format%20this%20way.-,Resizing%20Images,Image%20with%20the%20new%20dimensions.\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nx = []\ny = []\nfor dirname, _, filenames in os.walk('/kaggle/input/aptos2019-blindness-detection/train_images/'):\n    for filename in filenames:\n        # print(os.path.join(dirname, filename))\n        image = Image.open(os.path.join(dirname, filename))\n        x = x + [image.size[0]]\n        y = y + [image.size[1]]\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nfig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\nn_bins = 20\n# We can set the number of bins with the `bins` kwarg\naxs[0].hist(x, bins=n_bins)\naxs[1].hist(y, bins=n_bins)","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"(array([  44.,    0.,  287.,    0.,    0., 1066.,   77.,    0.,   28.,\n         351.,    0.,  639.,  567.,    0.,  410.,    0.,    0.,  141.,\n           0.,   52.]),\n array([ 358. ,  482.5,  607. ,  731.5,  856. ,  980.5, 1105. , 1229.5,\n        1354. , 1478.5, 1603. , 1727.5, 1852. , 1976.5, 2101. , 2225.5,\n        2350. , 2474.5, 2599. , 2723.5, 2848. ]),\n <a list of 20 Patch objects>)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing"},{"metadata":{},"cell_type":"markdown","source":"### Resize images and collecting diagnosis"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_rows, img_cols = 56, 56 # number of pixels \ntrain_data = pd.read_csv('/kaggle/input/aptos2019-blindness-detection/train.csv')\ntrain_images = []\ntrain_diagnosis = []\nfor dirname, _, filenames in os.walk('/kaggle/input/aptos2019-blindness-detection/train_images/'):\n    for filename in filenames:\n        image = Image.open(os.path.join(dirname, filename))\n        new_image = image.resize((img_rows, img_cols))\n        new_image.save('train_'+filename)\n        diagnosis = train_data.loc[train_data['id_code']+'.png' == filename,'diagnosis']\n        train_images = train_images + [new_image]\n        train_diagnosis = train_diagnosis + [diagnosis]","execution_count":28,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-2c0271c326af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mnew_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mnew_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdiagnosis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id_code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.png'\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'diagnosis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1908\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNEAREST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf # tensorflow 2.0\nfrom keras.datasets import mnist\nimport numpy as np\nseed=0\nnp.random.seed(seed) # fix random seed\ntf.random.set_seed(seed)\nnum_classes = 10 # 10 digits","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_images)","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"625"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\ntrain_diagnosis = keras.utils.to_categorical(np.array(train_diagnosis), 5)","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_diagnosis[0]","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"1091    3\nName: diagnosis, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()#add model layers\nmodel.add(Conv2D(32, kernel_size=(5, 5),\n                     activation='relu',\n                     input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n# add second convolutional layer with 20 filters\nmodel.add(Conv2D(64, (5, 5), activation='relu'))\n    \n# add 2D pooling layer\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n    \n# flatten data\nmodel.add(Flatten())\n    \n# add a dense all-to-all relu layer\nmodel.add(Dense(1024, activation='relu'))\n    \n# apply dropout with rate 0.5\nmodel.add(Dropout(0.5))\n    \n# soft-max layer\nmodel.add(Dense(num_classes, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#compile model using accuracy to measure model performance\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the model\nmodel.fit(train_images, train_diagnosis, validation_data=(test_images, test_diagnosis), epochs=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate the model\nscore = model.evaluate(test_images, test_diagnosis, verbose=1)\n\n# print performance\nprint()\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}